{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henrik/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "from keras.layers import *\n",
    "from keras import Model, Sequential, Input\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l2\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "\n",
    "\n",
    "def load(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "dataTriple = load('obj/triplets.pkl')\n",
    "dataPairs = load('obj/pairs.pkl')\n",
    "triplets = dataTriple['triplets'] / 256.\n",
    "pairs = dataPairs['pairs'] / 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_darknet():\n",
    "    model = load_model('darknet.h5')\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    return model\n",
    "\n",
    "def make_style_model():\n",
    "    darknet = get_darknet()\n",
    "    \n",
    "    # pretrained layers\n",
    "    inputs = darknet.layers[0].input\n",
    "    x = darknet.layers[16].output\n",
    "    \n",
    "    # new layers\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Lambda(lambda x: tf.nn.l2_normalize(x, axis=0))(x)\n",
    "    base_model = Model(inputs, x)\n",
    "    \n",
    "    # siamese networks\n",
    "    input_a = Input((256,256,3))\n",
    "    input_b = Input((256,256,3))\n",
    "    input_c = Input((256,256,3))\n",
    "    \n",
    "    encoding_a = base_model(input_a)\n",
    "    encoding_b = base_model(input_b)\n",
    "    encoding_c = base_model(input_c)\n",
    "    \n",
    "    dist_pos = Lambda(sqEucl, output_shape=(1,))([encoding_a, encoding_b])\n",
    "    dist_neg = Lambda(sqEucl, output_shape=(1,))([encoding_a, encoding_c])\n",
    "    triple_loss = Lambda(triplet_loss, output_shape=(1,))([dist_pos, dist_neg])\n",
    "    \n",
    "    triple_model = Model([input_a, input_b, input_c], triple_loss)\n",
    "    pairModel = Model([input_a, input_b], dist_pos)\n",
    "    \n",
    "    return triple_model, pairModel\n",
    "\n",
    "def triplet_loss(x):\n",
    "    return K.maximum(x[0] - x[1] + 1, 0)\n",
    "\n",
    "def sqEucl(x):\n",
    "    return K.sum(K.square(x[0] - x[1]), axis=-1, keepdims=True)\n",
    "\n",
    "def norm(x):\n",
    "    return K.sqrt(sqEucl(x))\n",
    "    \n",
    "def identity(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henrik/miniconda3/lib/python3.6/site-packages/keras/models.py:255: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 3.2055\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 1.1987\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 1.2221\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 1.1137\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.9760\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.8889\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.7815\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.7597\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.7242\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.5685\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.4829\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3821\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3634\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3066\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2507\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2578\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3675\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3550\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2809\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2347\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2181\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1839\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1739\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1513\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1397\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1302\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1192\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1151\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1150\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1072\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1020\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.0995\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1042\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.0989\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1013\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.0966\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.0906\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.0973\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.0886\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.0879\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.0881\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.0873\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.0867\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.0916\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.0864\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.0873\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.0827\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.0849\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.0809\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.0802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0eeb592e80>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tripleModel, pairModel = make_style_model()\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "tripleModel.compile(optimizer=sgd, loss=identity)\n",
    "pairModel.compile(optimizer=sgd, loss=identity)\n",
    "alphas = np.zeros(triplets.shape[0])\n",
    "\n",
    "\n",
    "tripleModel.fit([triplets[:, 0], triplets[:, 1], triplets[:, 2]], alphas, epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.4950957  5.1264925 ]\n",
      " [3.8152025  3.6399188 ]\n",
      " [0.9264819  4.3501234 ]\n",
      " [1.2057012  5.349226  ]\n",
      " [1.242723   2.6187909 ]\n",
      " [1.5210639  1.945425  ]\n",
      " [0.9375831  2.1345413 ]\n",
      " [0.8826487  4.4403906 ]\n",
      " [0.91110337 2.4546547 ]\n",
      " [2.7326872  3.8913617 ]]\n",
      "[[1.8716867  3.3427753 ]\n",
      " [0.69695514 3.4665778 ]\n",
      " [2.8598783  3.8320096 ]\n",
      " [1.3541038  4.5497518 ]\n",
      " [0.40543768 3.0174813 ]\n",
      " [1.4152956  2.8619452 ]\n",
      " [1.7945244  2.2570984 ]\n",
      " [0.968596   4.809819  ]\n",
      " [0.36610913 1.6447008 ]\n",
      " [0.32626522 4.873949  ]]\n",
      "[[2.7475417 3.8120337]\n",
      " [1.2986265 1.9739528]\n",
      " [0.7770302 1.641121 ]\n",
      " [2.4982455 3.863014 ]\n",
      " [1.0114396 1.766274 ]\n",
      " [1.0013199 3.2714286]\n",
      " [2.2748902 6.1354713]\n",
      " [2.1386037 3.5767896]\n",
      " [1.3487297 2.646108 ]\n",
      " [1.4915531 2.3894005]]\n",
      "[[0.793393   1.9127164 ]\n",
      " [2.5631135  6.202421  ]\n",
      " [1.4571168  4.110161  ]\n",
      " [1.0843265  2.3613775 ]\n",
      " [3.0512862  3.8455002 ]\n",
      " [2.2754607  2.260673  ]\n",
      " [1.0580051  3.600628  ]\n",
      " [0.44122577 5.1234956 ]\n",
      " [2.361735   1.9579437 ]\n",
      " [1.2477882  2.2233624 ]]\n",
      "[[1.4938849  2.784994  ]\n",
      " [0.81266487 3.2060585 ]\n",
      " [3.1570544  3.9823833 ]\n",
      " [1.0309646  2.1163287 ]\n",
      " [1.2849419  4.8387113 ]\n",
      " [0.69790286 2.2492046 ]\n",
      " [1.5061706  1.8798289 ]\n",
      " [1.6500564  0.9967065 ]\n",
      " [2.5185513  5.2545295 ]\n",
      " [1.1869557  2.177604  ]]\n",
      "[[1.5778091  2.3186727 ]\n",
      " [2.1816506  3.353367  ]\n",
      " [2.6069903  3.0308619 ]\n",
      " [1.1008879  1.9871154 ]\n",
      " [0.75727713 1.4890398 ]\n",
      " [1.5650873  4.9820127 ]\n",
      " [0.31113917 3.2059999 ]\n",
      " [1.0296652  3.2466035 ]\n",
      " [0.5361254  3.335052  ]\n",
      " [1.476414   1.5088619 ]]\n",
      "[[0.4257193 2.068375 ]\n",
      " [1.1479028 3.8018188]\n",
      " [1.6219548 1.9399025]\n",
      " [0.58436   2.4148378]\n",
      " [1.999145  5.953022 ]\n",
      " [0.646448  1.2745438]\n",
      " [0.8063671 4.507054 ]\n",
      " [2.7494185 4.5503764]\n",
      " [1.0645151 4.788338 ]\n",
      " [1.0015447 3.5969346]]\n",
      "[[0.82154727 2.0866468 ]\n",
      " [1.7045572  3.8612342 ]\n",
      " [1.5991489  6.026923  ]\n",
      " [1.4983747  7.390819  ]\n",
      " [2.7058194  4.1300855 ]\n",
      " [1.2065332  1.5069672 ]\n",
      " [1.3834667  3.0926073 ]\n",
      " [0.84674793 1.5809596 ]\n",
      " [0.913273   2.6044943 ]\n",
      " [0.5916735  4.119299  ]]\n",
      "[[2.1902456 5.239025 ]\n",
      " [2.192286  2.4631462]\n",
      " [0.9069597 5.1769047]\n",
      " [1.9675949 3.1342988]\n",
      " [1.4308147 3.5061865]\n",
      " [1.4952183 3.301219 ]\n",
      " [3.208814  7.8082123]\n",
      " [1.2719469 1.8668535]\n",
      " [1.3338112 2.9256048]\n",
      " [1.3745002 3.2973518]]\n",
      "[[1.6527624  4.411045  ]\n",
      " [1.3217617  1.433258  ]\n",
      " [2.7953258  3.304603  ]\n",
      " [2.0029714  2.8520207 ]\n",
      " [1.896764   5.405993  ]\n",
      " [2.7691169  2.4082594 ]\n",
      " [0.44505873 4.00544   ]\n",
      " [0.781314   2.1983204 ]\n",
      " [2.1988068  2.0606396 ]\n",
      " [3.0250595  6.2818985 ]]\n",
      "[[1.8971167  2.5832095 ]\n",
      " [0.87987363 4.041972  ]\n",
      " [1.1813594  4.3360953 ]\n",
      " [1.4405866  4.334913  ]\n",
      " [2.1798892  2.1386576 ]\n",
      " [1.8098632  5.038908  ]\n",
      " [0.88194776 2.1981244 ]\n",
      " [1.5679054  4.459485  ]\n",
      " [1.6000084  3.451192  ]\n",
      " [0.6437001  1.569431  ]]\n",
      "[[1.4591112  2.1967115 ]\n",
      " [1.1243823  2.1988513 ]\n",
      " [1.3548756  2.6792586 ]\n",
      " [2.420832   3.2629004 ]\n",
      " [4.827401   2.7582386 ]\n",
      " [0.60696745 2.439664  ]\n",
      " [0.9996331  4.395668  ]\n",
      " [1.6291779  1.9615445 ]\n",
      " [0.7532906  1.3053355 ]\n",
      " [1.8168064  3.6152208 ]]\n",
      "[[1.1150954  2.8434124 ]\n",
      " [0.7209591  2.8098536 ]\n",
      " [1.8271444  4.208236  ]\n",
      " [0.6233871  4.1413403 ]\n",
      " [0.3704252  5.436879  ]\n",
      " [1.0869739  4.889241  ]\n",
      " [0.24846256 2.857345  ]\n",
      " [1.1960362  5.1746078 ]\n",
      " [1.4906092  2.837276  ]\n",
      " [2.0415454  4.2801776 ]]\n",
      "[[0.83226985 2.041925  ]\n",
      " [1.1405032  5.9984293 ]\n",
      " [1.4112754  4.8055906 ]\n",
      " [1.640047   2.1314201 ]\n",
      " [1.1144459  6.2563624 ]\n",
      " [0.6632283  1.487577  ]\n",
      " [2.1868446  2.0825443 ]\n",
      " [1.9079726  2.5674086 ]\n",
      " [1.1932882  2.9132454 ]\n",
      " [0.56617785 2.0476418 ]]\n",
      "[[1.9399514  2.50414   ]\n",
      " [0.64419913 2.2029605 ]\n",
      " [1.4055054  2.009809  ]\n",
      " [1.4461656  3.8639972 ]\n",
      " [0.6434169  2.142435  ]\n",
      " [1.4488034  4.4917827 ]\n",
      " [1.2456834  3.0766287 ]\n",
      " [1.8420229  1.2422664 ]\n",
      " [2.3663213  5.6611843 ]\n",
      " [2.3116508  3.5141487 ]]\n",
      "[[1.9570551 2.9374135]\n",
      " [1.11797   3.105363 ]\n",
      " [0.7480036 6.056237 ]\n",
      " [1.3511583 1.9336171]\n",
      " [1.7246995 2.9665802]\n",
      " [2.2825942 1.8987565]\n",
      " [1.1100416 2.5769267]\n",
      " [2.0963032 4.521952 ]\n",
      " [2.3037639 3.3547492]\n",
      " [1.6973912 3.268284 ]]\n",
      "[[1.3152218  4.7266426 ]\n",
      " [0.6126535  2.8482592 ]\n",
      " [1.5036867  2.2598996 ]\n",
      " [1.2383478  3.0108674 ]\n",
      " [1.0809921  4.4107327 ]\n",
      " [2.0796943  4.4117103 ]\n",
      " [1.1249025  1.8058233 ]\n",
      " [1.4883652  3.265168  ]\n",
      " [0.38874927 2.128045  ]\n",
      " [1.0567893  1.8636358 ]]\n",
      "[[0.6862199  2.8529124 ]\n",
      " [1.7787336  4.297517  ]\n",
      " [0.98238075 5.24427   ]\n",
      " [1.4779166  3.7670155 ]\n",
      " [1.0673604  3.4001775 ]\n",
      " [0.542526   3.4476738 ]\n",
      " [2.2452698  1.887799  ]\n",
      " [0.8099992  2.6425362 ]\n",
      " [0.5962348  2.6207013 ]\n",
      " [1.6892877  3.1296852 ]]\n",
      "[[1.4105332  2.1417975 ]\n",
      " [2.83388    1.6929295 ]\n",
      " [0.72885644 3.158288  ]\n",
      " [1.7035891  2.6812384 ]\n",
      " [1.1397574  3.4773097 ]\n",
      " [0.68222326 2.168339  ]\n",
      " [0.51936823 2.1343493 ]\n",
      " [1.0554743  3.4444277 ]\n",
      " [0.6806507  2.9705732 ]\n",
      " [1.9278538  5.7024803 ]]\n",
      "[[1.0476489  3.503733  ]\n",
      " [3.171986   3.031424  ]\n",
      " [1.2209384  3.1007996 ]\n",
      " [1.2904353  3.865167  ]\n",
      " [2.5531764  3.0159893 ]\n",
      " [0.96815    2.1792812 ]\n",
      " [1.1158993  3.1634297 ]\n",
      " [0.6345816  3.0385725 ]\n",
      " [0.5961018  2.5323586 ]\n",
      " [0.60428196 2.8076525 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def predict(x, y):\n",
    "    return pairModel.predict([x, y]).ravel()\n",
    "\n",
    "for i in range(20):\n",
    "    begin = i*20\n",
    "    end = begin + 20\n",
    "    predictions = predict(pairs[begin:end,0], pairs[begin:end,1]).reshape((-1,2))\n",
    "    print(predictions.reshape((-1, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
